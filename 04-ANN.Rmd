# Kuliah 4

Entropy

```{r}
p <- 0.6919
E <- function(p) -p * log2(p) - (1-p) * log2(1-p)
E(p)
E(p)
```

Information Gain

```{r}
IG <- 
```

```{r}
info_gain <- function(D, A) {
  H_D <- H(D[[A]]) 
  
  values <- unique(D[[A]])
  D_v <- split(D, D[[A]]) 
  
  H_D_A <- sum(sapply(D_v, function(x) length(x)/nrow(D) * H(x[[A]]))) 
  
  return(H_D - H_D_A)
}

H <- function(D) {
  p <- table(D)/length(D) 
  return(sum(-p*log2(p))) 
}



# Data 
data <- data.frame(
  outcome = c("Yes","Yes","No","Yes","No","Yes"),
  attribute = c("Tall","Tall","Tall","Short","Short","Short")
)

# Hitung entropy sebelum split
H_D <- H(data$outcome) 
H_D

# Split data berdasarkan attribute
D_v <- split(data, data$attribute)

# Hitung weighted entropy setelah split
H_D_A <- sum(sapply(D_v, function(x) length(x)/nrow(data) * H(x$outcome)))
H_D_A

# Hitung information gain
info_gain(data, "attribute")
```

```{r}
# Tabel kontingensi  
table <- matrix(c(6,3,2,1), nrow = 2, byrow = TRUE)
table

# Hitung total data 
N <- sum(table)  

# Hitung entropy sebelum split
H_D <- sum(-table/N * log2(table/N))

# Buat tabel kontingensi terpisah untuk setiap atribut
table_A1 <- matrix(c(6,2), nrow = 1)
table_A2 <- matrix(c(3,1), nrow = 1)

# Hitung weighted entropy
H_A <- sum(table[,1]/N * apply(cbind(table_A1, table_A2), 1, function(x) sum(-x/sum(x)*log2(x/sum(x))))) 

# Hitung information gain
IG <- H_D - H_A
IG
```

```{r}
# Tabel kontingensi  
table <- matrix(c(561, 27,
                  51.75, 2.49,
                  95.41, 4.59,
                  74.8, 8.08,
                  189, 307), nrow = 2, byrow = TRUE)
table

# Hitung total data 
N <- sum(table)  

# Hitung entropy sebelum split
H_D <- sum(-table/N * log2(table/N))

# Buat tabel kontingensi terpisah untuk setiap atribut
table_A1 <- matrix(c(6,2), nrow = 1)
table_A2 <- matrix(c(3,1), nrow = 1)

# Hitung weighted entropy
H_A <- sum(table[,1]/N * apply(cbind(table_A1, table_A2), 1, function(x) sum(-x/sum(x)*log2(x/sum(x))))) 

# Hitung information gain
IG <- H_D - H_A
IG
```

Ilustrasi

```{r}
##Classification Tree
propensity <- read.csv("D:/datatree01.csv", sep=";", header=TRUE)
tertarik <- factor(propensity$Tertarik.Beli., levels = 0:1, labels = c("Tidak", "Tertarik")) 
jk <- factor(propensity$Jenis.Kelamin,   levels = 0:1, labels = c("Perempuan", "Laki-Laki")) 
kota <- factor(propensity$Tinggal.di.Kota, levels = 0:1, labels = c("Tidak", "Ya"))
single <- factor(propensity$Single, levels = 0:1, labels = c("Menikah", "Single")) 
merokok <- factor(propensity$Perokok, levels = 0:1, labels = c("Tidak", "Ya")) 
budget <- propensity$Budget
usia <-propensity$usia
```

```{r}
library(rpart)
model.01 <- rpart(tertarik ~ jk + kota + single + usia + merokok + budget,
method="class", control = rpart.control(minsplit = 100))
model.01
```

```{r}
library(rpart.plot)
rpart.plot(model.01, extra=6)
```

```{r}
rpart.plot(model.01, extra=1)
```

```{r}
model.02 <- rpart(tertarik ~ jk + kota + single + usia + merokok + budget,
method="class", control = rpart.control(minsplit = 50))
rpart.plot(model.02, extra=6)
```

```{r}
data.training <- data.frame(jk, kota, single, usia, merokok, budget, tertarik) 
prob.prediksi.02 <- predict(model.02, newdata=data.training)
head(prob.prediksi.02)
```

```{r}
prediksi.02 <- factor(ifelse(prob.prediksi.02[,2] > 0.5, 1, 0),
levels = 0:1, labels = c("Tidak", "Te r t a r i k "))


library(caret)
confusionMatrix(prediksi.02, tertarik)
```

```{r}
prediksi.02 <- factor(ifelse(prob.prediksi.02[,2] > 0.6, 1, 0),
levels = 0:1, labels = c("Tidak", "Te r t a r i k "))

confusionMatrix(prediksi.02, tertarik)
```

```{r}
prediksi.02 <- factor(ifelse(prob.prediksi.02[,2] > 0.3, 1, 0),
levels = 0:1, labels = c("Tidak", "Te r t a r i k "))

confusionMatrix(prediksi.02, tertarik)
```

# Praktikum 4

### Preparation

#### Data

```{r}
dt4 <- read.csv("https://raw.githubusercontent.com/Zen-Rofiqy/STA1382-TPM/main/Materi/Prak%2004/ObesityDataSet.csv", stringsAsFactors = TRUE)
datatable(dt4)
```

```{r}
str(dt4)
```

```{r}
skim(dt4)
```

#### Cek Missing Value

```{r}
colSums(is.na(dt4))
```

#### Encoding Peubah Kategorik

```{r message=FALSE, warning=FALSE}
# One-Hot Encoder
for(i in 1:(dim(dt4)[2] - 1)){
  if(is.factor(dt4[,i]) == TRUE){
    dt4[,i] <- to_categorical(as.integer(dt4[,i]) - 1)
  }
}

# Contoh Hasil One-Hot Encoder
head(dt4$MTRANS)
```

```{r}
table(dt4$NObeyesdad)
```

#### Splitting Data & Scaling

```{r}
set.seed(123)

# Train-Testing Split
train.index <- createDataPartition(dt4$NObeyesdad, p = 0.8, list = F)
train <- dt4[train.index, ]
test <- dt4[-train.index, ]

# Feature Scaling (Min-Max)
preprocessParams <- preProcess(train[, -17], method = "range")
train_X <- as.matrix(predict(preprocessParams, train[, -17]))
test_X <- as.matrix(predict(preprocessParams, test[, -17]))

# Encoding Variabel Respons
train_y <- to_categorical(as.integer(train[, 17])-1)
test_y <- to_categorical(as.integer(test[, 17])-1)
```

## Klasifikasi Multikelas

### Model 1 **(1 hidden layer)**

```{r message=FALSE, warning=FALSE}
# Arsitektur Model
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(train_X)) %>%
  layer_dense(units = ncol(train_y), activation = "softmax")

# Kompilasi Model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = c("accuracy")
)

print(model)
```

```{r message=FALSE, warning=FALSE}
# Training Model
history <- model %>% fit(
  train_X, train_y,
  shuffle = T,
  epochs = 100,
  batch_size = 32, 
  validation_split = 0.2
)
```

```{r dpi=300, fig.height = 7, fig.width = 10, fig.align = "center", message = FALSE, warning=FALSE}
plot(history)
```

```{r}
# Evaluasi Model dengan data Test
scores <- model %>% evaluate(test_X, test_y)
print(scores)
```

#### Prediksi Model

```{r}
# Prediksi dalam bentuk peluang
pred <- predict(model, test_X)
head(pred)
```

```{r}
# Prediksi Label
label_pred <- apply(pred, 1, which.max)
label_pred
```

```{r}
# Nilai aktual
label_true <- as.integer(test$NObeyesdad)
label_true
```

#### Evaluasi Model

```{r}
confusionMatrix(as.factor(label_true), as.factor(label_pred))
```

### Model 2 **(2 Hidden layers + dropout)**

```{r}
# Membuat model neural network dengan 2 hidden layer
model.2 <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = ncol(train_X)) %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(units = ncol(train_y), activation = "softmax")

# Mengkompilasi model
model.2 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = c("accuracy")
)

# Melakukan tahapan pelatihan model
history <- model.2 %>% fit(
  train_X, train_y,
  shuffle = T,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2, 
  verbose = F       # tidak menampilkan teks ouput pada setiap epoch
)
```

```{r}
print(model.2)
```

```{r dpi=300, fig.height = 7, fig.width = 10, fig.align = "center", message = FALSE, warning=FALSE}
plot(history)
```

```{r}
# Mengevaluasi model menggunakan data uji
scores <- model.2 %>% evaluate(test_X, test_y)
print(scores)
```

#### Prediksi Model

```{r}
prediksi <- predict(model.2, test_X)
head(prediksi)
```

Untuk menentukan kategori dengan peluang terbesar kita dapat menggunakan fungsi `which.max` yang dikombinasikan dengan fungsi `apply`.

```{r}
label_pred <- apply(prediksi, 1, which.max)
label_pred
```

```{r}
label_true <- as.integer(test$NObeyesdad)
label_true
```

#### Evaluasi Model

```{r}
confusionMatrix(as.factor(label_true), as.factor(label_pred))
```

#### Save Model

```{r message=FALSE, warning=FALSE}
# Menyimpan model dalam format HDF5
model.2 %>% save_model_hdf5("model_nn.h5")

# Memuat model dari file HDF5
model.2 <- load_model_hdf5("model_nn.h5")

print(model)
```

### Model 3 (**2 hidden layer + dropout + callback)**

```{r}
# Multiclass Model dengan 2 hidden layers + dropout + checkpoint + earlyStopping

model.3 <- keras_model_sequential() %>%
           layer_dense(units = 120, activation = "relu", input_shape = ncol(train_X)) %>%
           layer_dropout(0.2) %>%
           layer_dense(units = 100, activation = "relu") %>%
           layer_dropout(0.1) %>%
           layer_dense(units = ncol(train_y), activation = "softmax")


# menentukan nama dan path file untuk penyimpanan model
filepath <- "model_check_x.keras"

# mengatur kriteria checkpoint
# simpan model jika memperoleh skor terbaik
checkpoint <- callback_model_checkpoint(
    filepath=filepath,
    monitor="val_accuracy",
    save_best_only=TRUE,
    mode="max",
    verbose=1,
)

# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
    monitor="val_accuracy",
    patience=30,
    verbose=1,
)

# kompilasi model
model.3 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = c("accuracy")
)

# melatih model
history <- model.3 %>% fit(
  train_X, train_y,
  shuffle = T,
  epochs = 200,
  batch_size = 64,
  # validation_split = 0.2, # atau
  validation_data = list(test_X, test_y),
  callbacks=list(checkpoint, early_stopping),
)
```

```{r}
summary(model.3)
```

```{r dpi=300, fig.height = 7, fig.width = 10, fig.align = "center", message = FALSE, warning=FALSE}
plot(history)
```

```{r}
# evaluasi model
scores <- model.3 %>% evaluate(test_X, test_y)
print(scores)
```

#### **Memuat Model Hasil Checkpoint + Prediksi**

```{r}
# memuat model yang disimpan hasil checkpoint
last.model <- load_model_tf("model_check_x.keras")

pred.3 <- predict(last.model, test_X)

label_pred <- apply(pred.3, 1, which.max)
label_pred

# label_true <- apply(test_y, 1, which.max)
# label_true

confusionMatrix(as.factor(label_true), as.factor(label_pred))
```

## Klasifikasi Biner

Note: Agar lebih praktis, pada bagian ini akan kita gunakan dataset yang sama, namun targetnya adalah (Overweight vs Tidak Overweight)

```{r}
data.biner <- data.frame(dt4)

str(data.biner)
```

```{r}
to.binary <- function(level){
  is.overweight <- grepl("Overweight|Obesity", level)
  as.integer(is.overweight)
}

# Mengubah 4 kelas menjadi 2 kelas (0: Tidak Overweight, 1: Overweight)
data.biner$NObeyesdad <- sapply(data.biner$NObeyesdad, to.binary)

str(data.biner)
```

```{r}
table(data.biner$NObeyesdad)
```

```{r}
# Membagi data menjadi data latih dan data uji
set.seed(111)
train.index <- createDataPartition(data.biner$NObeyesdad, p = 0.7, list = FALSE)
train <- data.biner[train.index, ]
test <- data.biner[-train.index, ]

# Features Scaling MinMax (0, 1)
preprocessParams <- preProcess(train[, -17], method=c("range"))
train_X <- as.matrix(predict(preprocessParams, train[, -17]))
test_X <- as.matrix(predict(preprocessParams, test[, -17]))

# Peubah target sudah dalam bentuk 0 dan 1 
# (tidak perlu one-hot encoding)
train_y = train[, 17]
test_y = test[, 17]

head(train_y)
```

### Model 1 **(2 hidden layer + dropout + callback)**

```{r}
# Binary Model dengan 2 hidden layers + dropout + checkpoint + earlyStopping

model.bin <- keras_model_sequential() %>%
           layer_dense(units = 120, activation = "relu", input_shape = ncol(train_X)) %>%
           layer_dropout(0.2) %>%
           layer_dense(units = 120, activation = "relu") %>%
           layer_dropout(0.1) %>%
           layer_dense(units = 1, activation = "sigmoid")   # units = 1 dan fungsi aktivasi = sigmoid


# menentukan nama dan path file untuk penyimpanan model
filepath <- "model_bin_check.keras"

# mengatur kriteria checkpoint
# simpan model jika memperoleh skor terbaik
checkpoint <- callback_model_checkpoint(
    filepath=filepath,
    monitor="val_accuracy",
    save_best_only=TRUE,
    mode="max",
    verbose=1,
)

# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
    monitor="val_accuracy",
    patience=30,
    verbose=1,
)

# kompilasi model
model.bin %>% compile(
  loss = "binary_crossentropy",    # loss : binary_cross_entropy
  optimizer = "adam",
  metrics = c("accuracy")
)

# melatih model
history <- model.bin %>% fit(
  train_X, train_y,
  shuffle = T,
  epochs = 200,
  batch_size = 64,
  # validation_split = 0.2, # atau
  validation_data = list(test_X, test_y),
  callbacks=list(checkpoint, early_stopping),
)
```

```{r}
summary(model.bin)
```

```{r dpi=300, fig.height = 7, fig.width = 10, fig.align = "center", message = FALSE, warning=FALSE}
plot(history)
```

```{r}
# evaluasi model
scores <- model.bin %>% evaluate(test_X, test_y)
print(scores)
```

#### **Prediksi Data Uji**

```{r}
# memuat model yang disimpan hasil checkpoint
model.bin.check <- load_model_tf("model_bin_check.keras")

# prediksi
pred.bin <- predict(model.bin.check, test_X)

# karena hasilnya berupa nilai 0-1, dengan cut-off 0.5 kita bisa menggunakan fungsi round saja
label_pred <- round(pred.bin)     

confusionMatrix(as.factor(test_y), as.factor(label_pred))
```

## Model Regresi

```{r}
data.ab <-  read.csv("https://raw.githubusercontent.com/Zen-Rofiqy/STA1382-TPM/main/Materi/Prak%2004/abalone.csv", stringsAsFactors = TRUE)
datatable(data.ab)
```

```{r}
str(data.ab)
```

```{r}
skim(data.ab)
```

```{r}
# Cek Missing Value
colSums(is.na(data.ab))
```

### Feature Engineering

```{r}
# Menambahkan Kolom 'Age'
data.ab$Age <- data.ab$Rings + 1.5

# Menghapus Kolom 'Rings'
data.ab$Rings <- NULL

# One-Hot Encoding Kolom 'Sex'
data.ab$Sex <- to_categorical(as.integer(data.ab$Sex) - 1)
```

### Splitting & Scaling

```{r}
set.seed(123)

train.index <- createDataPartition(data.ab$Age, p = 0.8, list = FALSE)
train <- data.ab[train.index, ]
test <- data.ab[-train.index, ]

# Melakukan Feature Scaling min max (0, 1)
preprocessParams <- preProcess(train[, -9], method=c("range"))
train_X <- as.matrix(predict(preprocessParams, train[, -9]))
test_X <- as.matrix(predict(preprocessParams, test[, -9]))

train_y <- train[, 9]
test_y <- test[, 9]
```

### Pemodelan

```{r message=FALSE, warning=FALSE}
# Membuat model neural network dengan 2 hidden layer
model.reg <- keras_model_sequential() %>%
   layer_dense(units = 64, activation = "relu", input_shape = ncol(train_X)) %>%
   layer_dropout(0.2) %>%
   layer_dense(units = 64, activation = "relu") %>%
   layer_dropout(0.2) %>%
   layer_dense(units = 1, activation = "linear")      # units = 1 dan activation = "linear"

# menentukan nama dan path file untuk penyimpanan model
filepath <- "model_reg_check.keras"

# mengatur kriteria checkpoint
# simpan model jika memperoleh skor terbaik
checkpoint <- callback_model_checkpoint(
    filepath=filepath,
    monitor="val_loss",
    save_best_only=TRUE,
    mode="min",           # minimum loss
    verbose=1,
)

# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
    monitor="val_loss",
    patience=15,
    verbose=1,
)


# Kompilasi model
model.reg %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = list("mean_squared_error", "mean_absolute_error")
)
# Melakukan tahapan pelatihan model
history <- model.reg %>% fit(
  train_X, train_y,
  shuffle = T,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2,
  callbacks=list(checkpoint, early_stopping),
  verbose = F
)
```

```{r}
summary(model.reg)
```

```{r dpi=300, fig.height = 7, fig.width = 10, fig.align = "center", message = FALSE, warning=FALSE}
plot(history)
```

```{r}
# Mengevaluasi model menggunakan data uji
scores <- model.reg %>% evaluate(test_X, test_y)
print(scores)
```

```{r}
cat("RMSE:", scores[2]^0.5, "\n")
cat("MAE:", scores[3], "\n")
```

### Prediksi

```{r}
# Melakukan prediksi
prediksi <- predict(model.reg, test_X)

# menampilkan 10 hasil prediksi pertama
# disandingkan dengan nilai aslinya
head(cbind("True" = test_y, "Pred"= prediksi), 20)
```

### Evaluasi Model

```{r}
# Mengevaluasi model menggunakan data uji
scores <- model.reg %>% evaluate(test_X, test_y)
print(scores)
```

```{r}
keras_train <- model.reg %>% predict(train_X)
keras_test <- model.reg %>% predict(test_X)
# Training Evaluation
postResample(keras_train[,1], train$Age)
# Testing Evaluation
postResample(keras_test[,1], test$Age)
```

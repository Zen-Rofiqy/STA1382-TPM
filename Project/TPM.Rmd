---
title: "TPM"
author: "Angga Fathan Rofiqy"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  rmdformats::downcute:
    self_contained: true
    thumbnails: false
    lightbox: true
    gallery: true
    highlight: tango
    code_folding: show
    toc_depth: 3
    number_sections: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    fig_caption: true
pkgdown:
  as_is: true
---

```{r message=FALSE, warning=FALSE, include=FALSE}
#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {   

   # convert arguments to vector
   packages <- c(package1, ...)

   # start loop to determine if each package is installed
   for(package in packages){

       # if package is installed locally, load
       if(package %in% rownames(installed.packages()))
          do.call('library', list(package))

       # if package is not installed locally, download, then load
       else {
          install.packages(package)
          do.call("library", list(package))
       }
   } 
}

install_load("DT","dplyr","ggplot2","gridExtra","MASS","tree","rio","skimr",
             "tidyverse","kernlab","e1071","ISLR","RColorBrewer")

theme1.1 <- list(
  geom_hline(yintercept = 0, size = 1, colour="#333333"),
  theme(axis.text.x = element_text(angle = 45, hjust = 1, 
                                   margin = margin(b = 10, t=-20)),
        axis.text.y = element_text(vjust = 0.5, face = "bold", 
                                   margin = margin(l = 20, r = 0)),
        plot.title = element_text(hjust = 0.5, face = "bold"),
        text = element_text(size = 30),
        plot.subtitle = element_text(hjust = 0.5),
        panel.background = element_rect(fill = 'transparent'),
        plot.background = element_rect(fill='transparent', color=NA),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()
        ) 
)

#install.packages("tensorflow")
#install.packages("keras")

#library("reticulate")
#virtualenv_create("r-reticulate2", python = install_python())
#tensorflow::install_tensorflow()
#keras::install_keras()

# Delete Installation
#wunlink("~/.virtualenvs/r-tensorflow", recursive = TRUE)

library(tensorflow); library(keras); library(caret)

suppressMessages({
    library(e1071)
    library(caret)
})
```

```{r setup, include=FALSE, echo=FALSE, warning=FALSE, message = FALSE}
require("knitr")
opts_knit$set(root.dir = "C:/Users/Fathan/Documents/Obsidian Vault/2. Kuliah/Smt 6/2. Teknik Pembelajaran Mesin/Project")

#Export chart
export.chart <- "C:/Users/Fathan/Documents/Obsidian Vault/2. Kuliah/Smt 6/2. Teknik Pembelajaran Mesin/Project/Chart"
```

> <mark style="background-color: #91CCDB">**Code/Syntax :** [File.rmd](https://github.com/Zen-Rofiqy/STA1382-TPM/blob/main/TMP.Rmd) </mark>

# Kuliah 1

> **Penyiapan Data (R)**

## 1. **Statistik Deskriptif**

**Daftar variabel :**

-   `Pregnancies` : Number of times pregnant

-   `Glucose` : Plasma glucose concentration a 2 hours in an oral glucose tolerance test

-   `BloodPressure` : Diastolic blood pressure (mm Hg)

-   `SkinThickness` : Triceps skin fold thickness (mm)

-   `Insulin` : 2-Hour serum insulin (mu U/ml)

-   `BMI` : Body mass index (weight in kg/(height in m)\^2)

-   `DiabetesPedigreeFunction` : Diabetes pedigree function

-   `Age` : Age (years)

-   `Outcome` : Class variable (0 or 1) 268 of 768 are 1 (diabetes), the others are 0 (tidak diabetes)

```{r}

data <- read.csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')
summary(data)
head(data, 10)
```

```{r}
library(httr)

# Gantilah 'your_username' dan 'your_authkey' dengan informasi akun Kaggle Anda
username <- "zenrofiqy"
authkey <- "594416325ca5f1711fb40cb5e1795b8c"

url <- "https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/download?datasetVersionNumber=1"



# Lakukan permintaan GET dengan autentikasi
dataset <- httr::GET(url, httr::authenticate(username, authkey, type = "basic"))

# Simpan file CSV ke tempat sementara
temp <- tempfile(fileext = ".csv")
writeBin(dataset$content, temp)

# Baca data CSV langsung
data <- read.csv(temp)

# Hapus file sementara
file.remove(temp)

View(data)

```

# Kuliah 3

```{r}
str(cpus)
cpus.ltr <- tree(log10(perf) ~ syct + mmin + mmax + chmin + chmax, data=cpus)
plot(cpus.ltr)
text(cpus.ltr)

summary(cpus.ltr)
```

# Kuliah 4

Entropy

```{r}
p <- 0.6919
E <- function(p) -p * log2(p) - (1-p) * log2(1-p)
E(p)
E(p)
```

Information Gain

```{r}
IG <- 
```

```{r}
info_gain <- function(D, A) {
  H_D <- H(D[[A]]) 
  
  values <- unique(D[[A]])
  D_v <- split(D, D[[A]]) 
  
  H_D_A <- sum(sapply(D_v, function(x) length(x)/nrow(D) * H(x[[A]]))) 
  
  return(H_D - H_D_A)
}

H <- function(D) {
  p <- table(D)/length(D) 
  return(sum(-p*log2(p))) 
}



# Data 
data <- data.frame(
  outcome = c("Yes","Yes","No","Yes","No","Yes"),
  attribute = c("Tall","Tall","Tall","Short","Short","Short")
)

# Hitung entropy sebelum split
H_D <- H(data$outcome) 
H_D

# Split data berdasarkan attribute
D_v <- split(data, data$attribute)

# Hitung weighted entropy setelah split
H_D_A <- sum(sapply(D_v, function(x) length(x)/nrow(data) * H(x$outcome)))
H_D_A

# Hitung information gain
info_gain(data, "attribute")
```

```{r}
# Tabel kontingensi  
table <- matrix(c(6,3,2,1), nrow = 2, byrow = TRUE)
table

# Hitung total data 
N <- sum(table)  

# Hitung entropy sebelum split
H_D <- sum(-table/N * log2(table/N))

# Buat tabel kontingensi terpisah untuk setiap atribut
table_A1 <- matrix(c(6,2), nrow = 1)
table_A2 <- matrix(c(3,1), nrow = 1)

# Hitung weighted entropy
H_A <- sum(table[,1]/N * apply(cbind(table_A1, table_A2), 1, function(x) sum(-x/sum(x)*log2(x/sum(x))))) 

# Hitung information gain
IG <- H_D - H_A
IG
```

```{r}
# Tabel kontingensi  
table <- matrix(c(561, 27,
                  51.75, 2.49,
                  95.41, 4.59,
                  74.8, 8.08,
                  189, 307), nrow = 2, byrow = TRUE)
table

# Hitung total data 
N <- sum(table)  

# Hitung entropy sebelum split
H_D <- sum(-table/N * log2(table/N))

# Buat tabel kontingensi terpisah untuk setiap atribut
table_A1 <- matrix(c(6,2), nrow = 1)
table_A2 <- matrix(c(3,1), nrow = 1)

# Hitung weighted entropy
H_A <- sum(table[,1]/N * apply(cbind(table_A1, table_A2), 1, function(x) sum(-x/sum(x)*log2(x/sum(x))))) 

# Hitung information gain
IG <- H_D - H_A
IG
```

Ilustrasi

```{r}
##Classification Tree
propensity <- read.csv("D:/datatree01.csv", sep=";", header=TRUE)
tertarik <- factor(propensity$Tertarik.Beli., levels = 0:1, labels = c("Tidak", "Tertarik")) 
jk <- factor(propensity$Jenis.Kelamin,   levels = 0:1, labels = c("Perempuan", "Laki-Laki")) 
kota <- factor(propensity$Tinggal.di.Kota, levels = 0:1, labels = c("Tidak", "Ya"))
single <- factor(propensity$Single, levels = 0:1, labels = c("Menikah", "Single")) 
merokok <- factor(propensity$Perokok, levels = 0:1, labels = c("Tidak", "Ya")) 
budget <- propensity$Budget
usia <-propensity$usia
```

```{r}
library(rpart)
model.01 <- rpart(tertarik ~ jk + kota + single + usia + merokok + budget,
method="class", control = rpart.control(minsplit = 100))
model.01
```

```{r}
library(rpart.plot)
rpart.plot(model.01, extra=6)
```

```{r}
rpart.plot(model.01, extra=1)
```

```{r}
model.02 <- rpart(tertarik ~ jk + kota + single + usia + merokok + budget,
method="class", control = rpart.control(minsplit = 50))
rpart.plot(model.02, extra=6)
```

```{r}
data.training <- data.frame(jk, kota, single, usia, merokok, budget, tertarik) 
prob.prediksi.02 <- predict(model.02, newdata=data.training)
head(prob.prediksi.02)
```

```{r}
prediksi.02 <- factor(ifelse(prob.prediksi.02[,2] > 0.5, 1, 0),
levels = 0:1, labels = c("Tidak", "Te r t a r i k "))


library(caret)
confusionMatrix(prediksi.02, tertarik)
```

```{r}
prediksi.02 <- factor(ifelse(prob.prediksi.02[,2] > 0.6, 1, 0),
levels = 0:1, labels = c("Tidak", "Te r t a r i k "))

confusionMatrix(prediksi.02, tertarik)
```

```{r}
prediksi.02 <- factor(ifelse(prob.prediksi.02[,2] > 0.3, 1, 0),
levels = 0:1, labels = c("Tidak", "Te r t a r i k "))

confusionMatrix(prediksi.02, tertarik)
```

# Kuliah 5

### **Binary classification**

```{r}
library(neuralnet)
nn <- neuralnet(Species=="setosa" ~Petal.Length  +Petal.Width, iris, 
                linear.output = FALSE)
print(nn) 
plot(nn)
```

### **Multiclass classification**

```{r}
nn2 <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE) 
names(nn2)
nn2$act.fct 
print(nn2)
plot(nn2)
```

### **Custom activation function**

```{r}
softplus <- function(x) log(1 + exp(x))
nn3  <-  neuralnet((Species  ==  "setosa")  ~  Petal.Length  +  Petal.Width,  iris, 
linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)
print(nn3) 
plot(nn3)
```

## Kuis

```{r}
f.act <- function(x) log(1 + exp(x))

neuron <- function(x1, x2, x3){
  h1 = .5*x1 - .2*x2 - .1*x3
  h1 = f.act(h1)
  
  h2 = .3*x1 + .1*x2 + .6*x3
  h2 = f.act(h2)
  
  out = .2*h1 - .1*h2 
  out = f.act(out)
  return(out)
}

r1 <- neuron(1, 1, 2)
r2 <- neuron(2, 3, 1)
r3 <- neuron(2, 1, 1) 

y <- c(2, 3, 1)

y_ <- data.frame(r1, r2, r3)
print(y_)

n <- 3
mse <- 1/n * sum((y - y_)^2)
print(mse)
```

# Praktikum 5

## Maximal Margin Classifier

```{r}
set.seed(10)
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

Tujuan dari maximal margin classifier adalah untuk mengidentifikasi batas linier yang memaksimalkan total jarak antara garis dan titik terdekat di setiap kelas. Kita dapat menggunakan fungsi `svm()` dalam paket `e1071` untuk menemukan batasan ini.

Pada plot di atas, titik-titik yang diwakili oleh “X” adalah vektor pendukung, atau titik-titik yang secara langsung mempengaruhi garis klasifikasi. Titik yang ditandai dengan “o” adalah titik lainnya, yang tidak mempengaruhi perhitungan garis.

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit, dat)
```

## **Support Vector Classifier (SVC)**

```{r}
# Construct sample data set - not completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))

# Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

Dalam kasus data yang tidak dapat dipisahkan secara sempurna, argumen `cost =` menjadi sangat penting. Ini menghitung penalti yang terkait dengan pengamatan di sisi yang salah dari batas klasifikasi.

### Kernel Linier

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", cost = 10)
# Plot Results
plot(svmfit, dat)
```

### Kernel Polynomial

```{r}
svmfit2 <- svm(y~., data = dat, kernel = "polynomial", gamma = 2, cost = 10)
# Plot Results
plot(svmfit2, dat)
```

### Kernel Radial

```{r}
svmfit3 <- svm(y~., data = dat, kernel = "radial", gamma = 2)
# Plot Results
plot(svmfit3, dat)
```

### Kernel Sigmoid

```{r}
svmfit4 <- svm(y~., data = dat, kernel = "sigmoid", gamma = 2, cost = 10)
# Plot Results
plot(svmfit4, dat)
```

Dari keempat bentuk SVM di atas, terlihat SVM bisa membuat batas yang berbeda-beda, namun tidak semuanya baik dalam memisahkan data. Jadi jenis pemisah yang digunakan harus juga juga sesuai (Atau dicek kesesuainya dengan data yang ada).

## **Support Vector Machines (SVM)**

```{r}
# Generate some test data
set.seed (100)
x <- matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2

y <- c(rep(1,150),rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))

plot(x, col=y)
```

```{r}
set.seed(100)
train <- sample(200, 100)

svm.fit <- svm(y ~., data=dat[train,], kernel='radial', gamma=1, cost=1)

plot(svm.fit, dat[train,])
```

```{r}
summary(svm.fit)
```

```{r}
yhat <- predict(svm.fit, dat[-train,])
confusionMatrix(yhat, dat[-train,'y'])
```

Jika kita menaikkan nilai cost, maka kita akan menghasilkan kesalahan yang lebih kecil, namun hal tersebut juga berisiko overfitting.

```{r}
svm.fit <- svm(y ~., dat[train,], kernel='radial', gamma=1, cost=1e5)

plot(svm.fit, dat[train,])
```

```{r}
summary(svm.fit)
```

Kita juga dapat melakukan validasi silang. Kita dapat menggunakan perintah `tune()` untuk mencoba beberapa nilai cost yang berbeda serta beberapa nilai gamma yang berbeda agar sesuai dengan batas nonlinear.

```{r}
set.seed(100)
tune.out <- tune(svm, y ~., data=dat[train,], 
                 kernel='radial', 
                 ranges = list(cost=c(0.1,1,10,100,1000),
                 gamma=c(0.5, 1,2,3,4)))

summary(tune.out)
```

```{r}
# show best model
tune.out$best.model
```

Model yang paling banyak mengurangi kesalahan pada data pelatihan adalah yang menggunakan cost = 10 dan gamma= 0.5. Kemudian kita akan lihat bagaimana performa dari model dalam memprediksi kelas dari 100 data uji.

```{r}
yhat <- predict(tune.out$best.model, dat[-train,])
confusionMatrix(yhat, dat[-train, 'y'])
```

### ROC CURVES

```{r message=FALSE, warning=FALSE}
install_load("ROCR")

# function to handle the different models
rocplot <- function(pred, truth, ...){
  predob =  prediction(pred, truth)
  perf = performance(predob, 'tpr', 'fpr')
  plot(perf, ...)

}
```

Berikutnya kita akan membangun kembali SVM, kita tetapkan `decision.values=TRUE` untuk mendapatkan nilai yang sesuai.

```{r}
svm.opt <- svm(y ~., data=dat[train,], kernel='radial',
               gamma=2, cost=1, decision.values=T)

fitted <- attributes(predict(svm.opt, dat[train,], decision.values=T))$decision.values

rocplot(fitted, dat[train,'y'], main='Training Data')
```

## SVM dengan Package `caret`

```{r message=FALSE, warning=FALSE}
attach(Auto); str(Auto)
```

```{r}
datatable(Auto, filter = 'top', 
          options = list(pageLength = 5))
```

```{r}
# Create a binary variable that takes on 1 for cars with gas mileage > median
Auto$y <- NA
Auto$y[Auto$mpg > median(Auto$mpg)] <- 1
Auto$y[Auto$mpg <= median(Auto$mpg)] <- 0
Auto$y <- as.factor(Auto$y)
length(Auto[is.na(Auto$y)]) # make sure there are no NA's
```

### SVM dengan Kernel Linear

```{r}
set.seed(123)
split <- createDataPartition(y=Auto$y, p=0.7, list=FALSE)
train <- Auto[split,]
test <- Auto[-split,]
# Remove mpg / name features
train <- train[-c(1,9)]
test <- test[-c(1,9)]

# 10 fold cross validation
ctr <- trainControl(method='repeatedcv',
                    number=10,
                    repeats=3)

# Recall as C increases, the margin tends to get wider
grid <- data.frame(C=seq(0.01,10,0.5))

svm.linear <- train(y ~., train,
                 method='svmLinear',
                 preProc=c('center','scale'),
                 trControl=ctr,
                 tuneGrid=grid)
svm.linear
```

```{r}
svm.linear$bestTune
```

```{r}
ggplot(svm.linear)
```

```{r}
# Training error rate
confusionMatrix(predict(svm.linear, train), train$y)
```

```{r}
# Testing error rate
yhat <- predict(svm.linear, test)
confusionMatrix(yhat, test$y)
```

### SVM dengan Kernel Polinomial

```{r}
set.seed(123)
# Try a polynomial function
svm.poly <- train(y ~., train,
                 method='svmPoly',
                 trControl=ctr,
                 tuneLength=4)
                 
svm.poly
```

```{r}
svm.poly$bestTune
```

```{r}
plot(svm.poly)
```

```{r}
# Training error rate
confusionMatrix(predict(svm.poly, train), train$y)
```

```{r}
# Testing error rate
yhat <- predict(svm.poly, test)
confusionMatrix(yhat, test$y)
```

### SVM dengan Kernel Radial

```{r}
set.seed(123)

# Try a radial function
svm.radial <- train(y ~., train,
                 method='svmRadial',
                 trControl=ctr,
                 tuneLength=10)
svm.radial
```

```{r}
svm.radial$bestTune
```

```{r}
plot(svm.radial)
```

```{r}
# Training error rate
confusionMatrix(predict(svm.radial, train), train$y)
```

```{r}
# Testing error rate
yhat <- predict(svm.radial, test)
confusionMatrix(yhat, test$y)
```

## SVM (dari Kaggle)

### **Penyiapan Data**

> **Import data**

```{r}
data <- read.csv("https://raw.githubusercontent.com/sainsdataid/dataset/main/wine-quality-binary.csv")

str(data)

head(data)

# membuang kolom id
data$id <- NULL

# merubah kolom quality menjadi `factor`
data$quality <- as.factor(data$quality)

# melihat proporsi masing-masing kategori
quality <- data.frame(table(data$quality))

quality$prop <- round(quality$Freq/sum(quality$Freq), 3)

quality
```

> **Pembagian Data Latih dan Data Uji**

```{r}
# Membagi data Latih dan Data Uji

set.seed(123)  # Untuk menghasilkan nilai acak yang dapat direproduksi

# membagi data secara acak
# dengan menghasilkan indeks data latih dan data uji
# data latih = 75%, data uji = 25%
trainIndex <- createDataPartition(data$quality, p = 0.75, list = FALSE, times = 1)

# Buat data latih dan data uji berdasarkan indeks yang dihasilkan
data.train <- data[trainIndex, ]
data.test <- data[-trainIndex, ]

# melihat komposisi setiap kelas pada data train dan test
cbind("train" = table(data.train$quality), "test" = table(data.test$quality))
```

### Membuat Model SVM

```{r}
# Training SVM mod
svm.model <- svm(quality ~ ., data = data.train, kernel = "radial")
```

```{r}
# Prediksi menggunakan data uji
predictions <- predict(svm.model , newdata = data.test)

# Hitung akurasi
accuracy <- mean(predictions == data.test$quality)
print(paste("Akurasi:", accuracy))
```

```{r}
# untuk membuat confusion Matrix
confusionMatrix(data = predictions,
                reference = data.test$quality)
```

### Tuning Hiperparameter

```{r}
svm.tuned <- tune(svm, quality ~ ., 
                  data = data.train, 
                  kernel = "radial",  
                  ranges = list(cost = c( 0.1, 0.2, 0.5, 0.9, 1, 2, 10),
                                gamma=c(0.01,0.02, 0.05, 0.1)))

svm.tuned

(svm.best <- svm.tuned$best.model)

# Evaluasi model terbaik
best_predictions <- predict(svm.best, newdata = data.test)

# untuk membuat confusion Matrix
confusionMatrix(data = best_predictions,
                reference = data.test$quality)
```

```{r}
# Tuning hyperparameter (misalnya, mencari nilai C terbaik)
svm.tuned <- tune(svm, quality ~ ., data = data.train, 
                  kernel = "polynomial",  
                  cross = 3,
                  ranges = list(cost = c( 0.1, 0.2, 0.5, 0.9, 1, 2, 10),
                                gamma=c(0.01,0.02, 0.05, 0.1),
                                degree = c(1, 2, 3, 4))) # 1 sama dengan kernel linear

svm.tuned

(svm.best <- svm.tuned$best.model)

# Evaluasi model terbaik
best_predictions <- predict(svm.best, newdata = data.test)

# untuk membuat confusion Matrix
confusionMatrix(data = best_predictions,
                reference = data.test$quality)
```

## SUPPORT VECTOR REGRESSION (SVR)

### Penyiapan data

> **Load Data**

```{r}
# Membaca data
data <- read.csv("https://raw.githubusercontent.com/sainsdataid/dataset/main/data_ipm_jawa_2018.csv")

# Kolom ID tidak diperlukan
data$id = NULL

# Melihat Struktur data
str(data)
```

> **Pembagian Data Latih dan Data Uji**

```{r}
set.seed(123)

index <- createDataPartition(data$ipm_2018, p =  0.7, list = FALSE)

# Membagi dataset
train_data <- data[index, ]
test_data <- data[-index, ]
```

### Membuat Model SVR

```{r}
# Training SVM model untuk regresi
svr.model <- svm(ipm_2018 ~ ., data = train_data, kernel = "radial", type = "eps-regression")

# Prediksi menggunakan data uji
predictions <- predict(svr.model, newdata = test_data)

# Hitung Mean Squared Error (MSE)
rmse <- mean((predictions - test_data$ipm_2018)^2)^0.5
print(paste("Root Mean Squared Error (RMSE):", rmse))
```

### Tuning Hiperparameter

```{r}
# Tuning hyperparameter
svr.tuned <- tune(svm, ipm_2018 ~ ., 
                  data = train_data, 
                  kernel = "radial", 
                  type = "eps-regression",  
                  ranges = list(cost = c( 0.1, 0.2, 0.5, 0.9, 1, 2, 10),
                                gamma=c(0.01,0.02, 0.05, 0.1)))

# Model terbaik dari tuning
svr.best <- svr.tuned $best.model

# Evaluasi model terbaik
best_predictions <- predict(svr.best, newdata = test_data)
best_rmse <- mean((best_predictions- test_data$ipm_2018)^2)^0.5
print(paste("Mean Squared Error (MSE) Model Terbaik:", best_rmse))
```

# Referensi

## Kuliah/Praktikum

-   Praktikum 1 (Penyiapan Data) :

    -   <https://www.kaggle.com/code/cahyaalkahfi/sta1382-penyiapan-data-r>

-   Praktikum 2 (Model Regresi Linear & Logistik) :

    -   <https://www.kaggle.com/code/cahyaalkahfi/sta1382-model-regresi-linear-logistik>

-   Praktikum 3 (Pohon Klasifikasi [CART]) :

    -   <https://rpubs.com/rizkia/sta1382-pohon-klasifikasi>

-   Praktikum 4 (ANN) :

    -   <https://rpubs.com/rizkia/sta1382-neural-network>

    -   ANN Regression with Keras : <https://www.kaggle.com/code/cahyaalkahfi/ann-untuk-regresi-dengan-keras-r>

    -   ANN Model Klasifikasi dengan Keras : <https://www.kaggle.com/code/cahyaalkahfi/ann-model-klasifikasi-dengan-keras-r>

<!-- -->

-   Praktikum 5 (SVM) :

    -   <https://rpubs.com/rizkia/sta1382-svm>

    -   [https://www.kaggle.com/code/cahyaalkahfi/support-vector-machines-svm-dengan-r#B.-SUPPORT-VECTOR-REGRESSION-(SVR)](https://www.kaggle.com/code/cahyaalkahfi/support-vector-machines-svm-dengan-r#B.-SUPPORT-VECTOR-REGRESSION-(SVR)){.uri}

## Luar

-   Alkahfi, C. (Feb 19, 2023). Model Neural Network pada R Menggunakan Library Keras. Retrieved from: <https://sainsdata.id/uncategorized/2630/model-neural-network-pada-r-menggunakan-library-keras/>

-   Keras R Documentation : <https://www.rdocumentation.org/packages/keras/versions/2.11.0>

-   Pichler, M. (June 6, 2018). An Introduction to machine learning with Keras in R. Retrieved from: <https://www.r-bloggers.com/2018/06/an-introduction-to-machine-learning-with-keras-in-r/>

-   Sadik, K. (2022). Artificial Neural Network [Slide Kuliah STA1582 Pembelajaran Mesin Statistika IPB University]

-   Sallan, J. M. (May 17, 2020). Neural networks. Retrieved from: <https://rpubs.com/jmsallan/nn_intro>

-   <https://sainsdata.id/machine-learning/2630/model-neural-network-pada-r-menggunakan-library-keras/#model-klasifikasi-dengan-neural-network>

-   Folley, M. (Dec 5, 2022). Support Vector Machines. Retrieved from: <https://bookdown.org/mpfoley1973/supervised-ml/support-vector-machines.html#support-vector-machines-1>

-   Kelly, R. (July 14, 2012). Support Vector Machines. Retrieved from: <https://rpubs.com/ryankelly/svm>

-   Support Vector Machines. Retrieved from: <http://uc-r.github.io/svm>

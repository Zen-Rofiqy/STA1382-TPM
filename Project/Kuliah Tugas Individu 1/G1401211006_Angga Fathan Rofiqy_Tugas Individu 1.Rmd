---
title: "TMP"
author: "Angga Fathan Rofiqy"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  rmdformats::downcute:
    self_contained: true
    thumbnails: false
    lightbox: true
    gallery: true
    highlight: tango
    code_folding: show
    toc_depth: 3
    number_sections: false
    toc_float:
      collapsed: true
      smooth_scroll: true
    fig_caption: true
pkgdown:
  as_is: true
---

```{r setup, include=FALSE, echo=FALSE, warning=FALSE, message = FALSE}
path <- function() gsub  ( "\\\\",  "/",  readClipboard ()  )

require("knitr")
opts_knit$set(root.dir = "C:/Users/Fathan/Documents/Obsidian Vault/2. Kuliah/Smt 6/2. Teknik Pembelajaran Mesin/Project/Kuliah Tugas Individu 1")

#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {   

   # convert arguments to vector
   packages <- c(package1, ...)

   # start loop to determine if each package is installed
   for(package in packages){

       # if package is installed locally, load
       if(package %in% rownames(installed.packages()))
          do.call('library', list(package))

       # if package is not installed locally, download, then load
       else {
          install.packages(package)
          do.call("library", list(package))
       }
   } 
}

#install.packages("tensorflow")
#install.packages("keras")

#library("reticulate")
#virtualenv_create("r-reticulate2", python = install_python())
tensorflow::install_tensorflow()
#keras::install_keras()

# Delete Installation
#wunlink("~/.virtualenvs/r-tensorflow", recursive = TRUE)

install_load("DT","dplyr","ggplot2","gridExtra","MASS","neuralnet","tensorflow"
             ,"keras","caret","lattice","reticulate")
```

# Pendahuluan

## Deskripsi Tugas

Suatu perusahaan perbank-kan meneliti 75 jenis skema pinjaman yang telah diberi rating oleh para customernya pada `data ann.csv`.

Variabel yang digunakan ialah:

-   Besar pinjaman (dalam juta rupiah)

-   Lama pembayaran (dalam tahun)

-   Tambahan bunga yang ditetapkan (dalam %)

-   Pembayaran per bulan (dalam 10000)

-   Banyak cash back yang diterapkan pada skema tersebut

**Tujuan** penelitian yang dilakukan ialah **memprediksi rating skema pinjaman** berdasarkan variabel-variabel tersebut. Bantulah peneliti tersebut untuk memprediksi reating skema pinjaman dengan **neural network**, hitunglah **RMSE prediksinya**.

> **Ketentuan Tugas**

-   Gunakan **data ann.csv**

-   Kerjakan prediksi rating skema pinjaman dengan neural network, hitunglah RMSE prediksinya

-   Kirimkan kode R beserta output dan interpretasinya pada file dengan format pdf

-   Batas waktu pengiriman adalah **Hari Senin, tanggal 26 Februari 2024 jam 13:00 WIB** 

## Data

```{r}
data <- read.csv("data ann.csv")
#Data Type
str(data)
```

Semua peubah merupakan peubah numerik, tidak ada yang perlu diubah.

## Missing Value

```{r}
colSums(is.na(data))
```

Terlihat bahwa tidak ada missing value.

# Model Regresi - Tanh

## Splitting Data & Scaling

Data perlu dilakukan scaling, agar skala dari setiap peubah sama.

```{r}
set.seed(123)

# Train-Testing Split
train.index <- createDataPartition(data$rating, p = 0.8, list = FALSE)
train <- data[train.index, ]
test <- data[-train.index, ]

# Melakukan Feature Scaling min max (0, 1)
preprocessParams <- preProcess(train[, -ncol(data)], method=c("range"))
train_X <- as.matrix(predict(preprocessParams, train[, -ncol(data)]))
test_X <- as.matrix(predict(preprocessParams, test[, -ncol(data)]))

train_y <- train[, ncol(data)]
test_y <- test[, ncol(data)]
```

## Pemodelan

```{r message=FALSE, warning=FALSE, echo = FALSE}
# Membuat model neural network 
model.tanh <- keras_model_sequential() %>%
  #Input
  layer_dense(units = 128, activation = "tanh", input_shape = ncol(train_X)) %>%
  layer_dropout(0.3) %>%
  #Hidden layer 1
  layer_dense(units = 128, activation = "tanh") %>%
  layer_dropout(0.3) %>%
  #Hidden layer 2
  layer_dense(units = 128, activation = "tanh") %>%
  layer_dropout(0.3) %>%
  #Output
  layer_dense(units = 1, activation = "linear")

# Mengkompilasi model
model.tanh %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = list("mean_squared_error", "mean_absolute_error")
)

# Melakukan tahapan pelatihan model
history.tanh <- model.tanh %>% fit(
  train_X, train_y,
  shuffle = T,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2
)
```

```{r}
print(model.tanh)
```

Dari hasil di atas, model ANN memiliki 2 hidden layer. Berikut merupakan informasi lengkapnya:

1.  Model terdiri dari 3 lapisan Dense (fully connected) diikuti oleh Dropout.

2.  Input Layer memiliki 64 unit dengan total parameter 384 (64\*input_dim + 64 bias units)

3.  Dropout pertama tidak menambahkan parameter apa pun. Ini hanya men-dropout sebagian unit saat training.

4.  Lapisan Dense kedua memiliki 64 unit dengan total parameter 4160 (64\*64 + 64 bias units)

5.  Dropout kedua tidak menambahkan parameter lagi.

6.  Lapisan Dense ketiga berfungsi sebagai output layer, memiliki 1 unit untuk regresi. Total parameter 65 (64\*1 + 1 bias unit)

7.  Secara keseluruhan model memiliki 4609 parameter latih yang dapat dipelajari melalui backpropagation gradient descent.

8.  Model menerapkan regularization berupa dropout untuk mencegah overfitting ke data latih.

9.  Arsitektur model cukup sederhana dengan 3 lapisan dense tetapi dropout membantunya menangani overfitting dengan baik.

> **Plot History**

```{r dpi=300, fig.height = 15, fig.width = 10, fig.align = "center", message = FALSE, warning=FALSE}
plot(history.tanh)
```

Overfitting terjadi ketika data training terus menurun sedangkan validasi malah menaik. Ketiga plot di atas menunjukkan bahwa model tidak mengalami overfitting. Ini berarti bisa dicobakan dengan `layer_dropout` yang lebih kecil.

## Prediksi

```{r}
# Evaluasi Model dengan data Test
prediksi.tanh <- predict(model.tanh, test_X)
```

```{r}
datatable(prediksi.tanh, filter = 'top', 
          options = list(pageLength = 6))
```

Berikut merupakan hasil prediksi dari model ANN - tanh.

## Evaluasi Model

```{r}
# Mengevaluasi model menggunakan data uji
scores.tanh <- model.tanh %>% evaluate(test_X, test_y)
print(scores.tanh)
```

MSE dan MAE cukup tinggi, mengindikasikan error prediksi yang besar

```{r}
keras_train.tanh <- model.tanh %>% predict(train_X)
keras_test.tanh <- model.tanh %>% predict(test_X)
# Training Evaluation
postResample(keras_train.tanh[,1], train$rating)
# Testing Evaluation
postResample(keras_test.tanh[,1], test$rating)
```

Model dengan 3 hidden layer dengan 128 unit per layer nya yang dikombinasikan dengan fungsi aktifasi **tanh** menghasilkan RMSE sebesar 15.87. Dikatakan bahwa fungsi aktivasi Sigmoid & Tanh Kurang direkomendasikan karena efek "vanishing gradient", tapi kadang digunakan juga.

# Model Regresi - ReLU

## Pemodelan

```{r message=FALSE, warning=FALSE, echo = FALSE}
# Membuat model neural network dengan 2 hidden layer
model2 <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = ncol(train_X)) %>%
  layer_dropout(0.3) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(0.3) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(0.3) %>%
  layer_dense(units = 1, activation = "linear")

# Mengkompilasi model
model2 %>% compile(
  loss = "mean_squared_error",
  optimizer = "adam",
  metrics = list("mean_squared_error", "mean_absolute_error")
)

# Melakukan tahapan pelatihan model
history2 <- model2 %>% fit(
  train_X, train_y,
  shuffle = T,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2
)
```

```{r}
print(model2)
```

```{r dpi=300, fig.height = 15, fig.width = 10, fig.align = "center", message = FALSE, warning=FALSE}
plot(history2)
```

## Prediksi

```{r}
# Evaluasi Model dengan data Test
prediksi2 <- predict(model2, test_X)
```

```{r}
head(prediksi2)
```

## Evaluasi Model

```{r}
# Mengevaluasi model menggunakan data uji
scores2 <- model2 %>% evaluate(test_X, test_y)
print(scores2)
```

```{r}
keras_train2 <- model2 %>% predict(train_X)
keras_test2 <- model2 %>% predict(test_X)
# Training Evaluation
postResample(keras_train2[,1], train$rating)
# Testing Evaluation
postResample(keras_test2[,1], test$rating)
```

Model dengan 3 hidden layer dengan 128 unit per layer nya yang dikombinasikan dengan fungsi aktifasi **ReLU** menghasilkan RMSE yang lebih kecil yakni 6.972. Dikatakan bahwa ReLU Sering menjadi pilihan default untuk hidden layer pada model regresi. Menghasilkan non-linearitas tanpa efek "vanishing gradient".

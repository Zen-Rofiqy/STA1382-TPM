callbacks=list(checkpoint, early_stopping),
)
summary(model.bin)
# evaluasi model
scores <- model.bin %>% evaluate(test_X, test_y)
print(scores)
plot(history)
# memuat model yang disimpan hasil checkpoint
model.bin.check <- load_model_tf("model_bin_check.keras")
# prediksi
pred.bin <- predict(model.bin.check, test_X)
# karena hasilnya berupa nilai 0-1, dengan cut-off 0.5 kita bisa menggunakan fungsi round saja
label_pred <- round(pred.bin)
confusionMatrix(as.factor(test_y), as.factor(label_pred))
# Menambahkan Kolom 'Age'
data.ab$Age <- data.ab$Rings + 1.5
data.ab <-  read.csv("https://raw.githubusercontent.com/Zen-Rofiqy/STA1382-TPM/main/Materi/Prak%2004/abalone.csv", stringsAsFactors = TRUE)
datatable(data.ab)
str(data.ab)
# evaluasi model
scores <- model.bin %>% evaluate(test_X, test_y)
skim(data.ab)
# Menambahkan Kolom 'Age'
data.ab$Age <- data.ab$Rings + 1.5
# Menghapus Kolom 'Rings'
data.ab$Rings <- NULL
# One-Hot Encoding Kolom 'Sex'
data.ab$Sex <- to_categorical(as.integer(data.ab$Sex) - 1)
set.seed(123)
train.index <- createDataPartition(data.ab$Age, p = 0.8, list = FALSE)
train <- data.ab[train.index, ]
test <- data.ab[-train.index, ]
# Melakukan Feature Scaling min max (0, 1)
preprocessParams <- preProcess(train[, -9], method=c("range"))
train_X <- as.matrix(predict(preprocessParams, train[, -9]))
test_X <- as.matrix(predict(preprocessParams, test[, -9]))
train_y <- train[, 9]
test_y <- test[, 9]
# Membuat model neural network dengan 2 hidden layer
model.reg <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(train_X)) %>%
layer_dropout(0.2) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units = 1, activation = "linear")      # units = 1 dan activation = "linear"
# menentukan nama dan path file untuk penyimpanan model
filepath <- "model_reg_check.keras"
# mengatur kriteria checkpoint
# simpan model jika memperoleh skor terbaik
checkpoint <- callback_model_checkpoint(
filepath=filepath,
monitor="val_loss",
save_best_only=TRUE,
mode="min",           # minimum loss
verbose=1,
)
# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
monitor="val_loss",
patience=15,
verbose=1,
)
# Kompilasi model
model.reg %>% compile(
loss = "mean_squared_error",
optimizer = "adam",
metrics = list("mean_squared_error", "mean_absolute_error")
)
# Melakukan tahapan pelatihan model
history <- model.reg %>% fit(
train_X, train_y,
shuffle = T,
epochs = 100,
batch_size = 32,
validation_split = 0.2,
callbacks=list(checkpoint, early_stopping),
verbose = F
)
# Mengevaluasi model menggunakan data uji
scores <- model.reg %>% evaluate(test_X, test_y)
print(scores)
summary(model.reg)
plot(history)
plot(history)
cat("RMSE:", scores[2]^0.5, "\n")
cat("MAE:", scores[3], "\n")
# Melakukan prediksi
prediksi <- predict(model.reg, test_X)
# menampilkan 10 hasil prediksi pertama
# disandingkan dengan nilai aslinya
head(cbind("True" = test_y, "Pred"= prediksi), 20)
# Mengevaluasi model menggunakan data uji
scores <- model.reg %>% evaluate(test_X, test_y)
print(scores)
keras_train <- model.reg %>% predict(train_X)
keras_test <- model.reg %>% predict(test_X)
# Training Evaluation
postResample(keras_train[,1], train$Age)
# Testing Evaluation
postResample(keras_test[,1], test$Age)
#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {
# convert arguments to vector
packages <- c(package1, ...)
# start loop to determine if each package is installed
for(package in packages){
# if package is installed locally, load
if(package %in% rownames(installed.packages()))
do.call('library', list(package))
# if package is not installed locally, download, then load
else {
install.packages(package)
do.call("library", list(package))
}
}
}
install_load("DT","dplyr","ggplot2","gridExtra","MASS","tree","rio","skimr")
theme1.1 <- list(
geom_hline(yintercept = 0, size = 1, colour="#333333"),
theme(axis.text.x = element_text(angle = 45, hjust = 1,
margin = margin(b = 10, t=-20)),
axis.text.y = element_text(vjust = 0.5, face = "bold",
margin = margin(l = 20, r = 0)),
plot.title = element_text(hjust = 0.5, face = "bold"),
text = element_text(size = 30),
plot.subtitle = element_text(hjust = 0.5),
panel.background = element_rect(fill = 'transparent'),
plot.background = element_rect(fill='transparent', color=NA),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()
)
)
#install.packages("tensorflow")
#install.packages("keras")
#library("reticulate")
#virtualenv_create("r-reticulate2", python = install_python())
#tensorflow::install_tensorflow()
#keras::install_keras()
# Delete Installation
#wunlink("~/.virtualenvs/r-tensorflow", recursive = TRUE)
library(tensorflow); library(keras); library(caret)
suppressMessages({
library(e1071)
library(caret)
})
#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {
# convert arguments to vector
packages <- c(package1, ...)
# start loop to determine if each package is installed
for(package in packages){
# if package is installed locally, load
if(package %in% rownames(installed.packages()))
do.call('library', list(package))
# if package is not installed locally, download, then load
else {
install.packages(package)
do.call("library", list(package))
}
}
}
install_load("DT","dplyr","ggplot2","gridExtra","MASS","tree","rio","skimr",
"tidyverse","kernlab","e1071","ISLR","RColorBrewer")
theme1.1 <- list(
geom_hline(yintercept = 0, size = 1, colour="#333333"),
theme(axis.text.x = element_text(angle = 45, hjust = 1,
margin = margin(b = 10, t=-20)),
axis.text.y = element_text(vjust = 0.5, face = "bold",
margin = margin(l = 20, r = 0)),
plot.title = element_text(hjust = 0.5, face = "bold"),
text = element_text(size = 30),
plot.subtitle = element_text(hjust = 0.5),
panel.background = element_rect(fill = 'transparent'),
plot.background = element_rect(fill='transparent', color=NA),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()
)
)
#install.packages("tensorflow")
#install.packages("keras")
#library("reticulate")
#virtualenv_create("r-reticulate2", python = install_python())
#tensorflow::install_tensorflow()
#keras::install_keras()
# Delete Installation
#wunlink("~/.virtualenvs/r-tensorflow", recursive = TRUE)
library(tensorflow); library(keras); library(caret)
suppressMessages({
library(e1071)
library(caret)
})
set.seed(10)
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))
# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) +
geom_point(size = 2) +
scale_color_manual(values=c("#000000", "#FF0000")) +
theme(legend.position = "none")
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit, dat)
# Construct sample data set - not completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))
# Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) +
geom_point(size = 2) +
scale_color_manual(values=c("#000000", "#FF0000")) +
theme(legend.position = "none")
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", cost = 10)
# Plot Results
plot(svmfit, dat)
svmfit2 <- svm(y~., data = dat, kernel = "polynomial", gamma = 2, cost = 10)
# Plot Results
plot(svmfit2, dat)
require("knitr")
opts_knit$set(root.dir = "C:/Users/Fathan/Documents/Obsidian Vault/2. Kuliah/Smt 6/2. Teknik Pembelajaran Mesin/Project")
#Export chart
export.chart <- "C:/Users/Fathan/Documents/Obsidian Vault/2. Kuliah/Smt 6/2. Teknik Pembelajaran Mesin/Project/Chart"
#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {
# convert arguments to vector
packages <- c(package1, ...)
# start loop to determine if each package is installed
for(package in packages){
# if package is installed locally, load
if(package %in% rownames(installed.packages()))
do.call('library', list(package))
# if package is not installed locally, download, then load
else {
install.packages(package)
do.call("library", list(package))
}
}
}
install_load("DT","dplyr","ggplot2","gridExtra","MASS","tree","rio","skimr",
"tidyverse","kernlab","e1071","ISLR","RColorBrewer")
theme1.1 <- list(
geom_hline(yintercept = 0, size = 1, colour="#333333"),
theme(axis.text.x = element_text(angle = 45, hjust = 1,
margin = margin(b = 10, t=-20)),
axis.text.y = element_text(vjust = 0.5, face = "bold",
margin = margin(l = 20, r = 0)),
plot.title = element_text(hjust = 0.5, face = "bold"),
text = element_text(size = 30),
plot.subtitle = element_text(hjust = 0.5),
panel.background = element_rect(fill = 'transparent'),
plot.background = element_rect(fill='transparent', color=NA),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()
)
)
#install.packages("tensorflow")
#install.packages("keras")
#library("reticulate")
#virtualenv_create("r-reticulate2", python = install_python())
#tensorflow::install_tensorflow()
#keras::install_keras()
# Delete Installation
#wunlink("~/.virtualenvs/r-tensorflow", recursive = TRUE)
library(tensorflow); library(keras); library(caret)
suppressMessages({
library(e1071)
library(caret)
})
require("knitr")
opts_knit$set(root.dir = "C:/Users/Fathan/Documents/Obsidian Vault/2. Kuliah/Smt 6/2. Teknik Pembelajaran Mesin/Project")
#Export chart
export.chart <- "C:/Users/Fathan/Documents/Obsidian Vault/2. Kuliah/Smt 6/2. Teknik Pembelajaran Mesin/Project/Chart"
set.seed(10)
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))
# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) +
geom_point(size = 2) +
scale_color_manual(values=c("#000000", "#FF0000")) +
theme(legend.position = "none")
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit, dat)
# Construct sample data set - not completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))
# Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) +
geom_point(size = 2) +
scale_color_manual(values=c("#000000", "#FF0000")) +
theme(legend.position = "none")
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", cost = 10)
# Plot Results
plot(svmfit, dat)
svmfit2 <- svm(y~., data = dat, kernel = "polynomial", gamma = 2, cost = 10)
# Plot Results
plot(svmfit2, dat)
svmfit3 <- svm(y~., data = dat, kernel = "radial", gamma = 2)
# Plot Results
plot(svmfit3, dat)
svmfit4 <- svm(y~., data = dat, kernel = "sigmoid", gamma = 2, cost = 10)
# Plot Results
plot(svmfit4, dat)
# Generate some test data
set.seed (100)
x <- matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y <- c(rep(1,150),rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))
plot(x, col=y)
set.seed(100)
train <- sample(200, 100)
svm.fit <- svm(y ~., data=dat[train,], kernel='radial', gamma=1, cost=1)
plot(svm.fit, dat[train,])
summary(svm.fit)
yhat <- predict(svm.fit, dat[-train,])
confusionMatrix(yhat, dat[-train,'y'])
svm.fit <- svm(y ~., dat[train,], kernel='radial', gamma=1, cost=1e5)
plot(svm.fit, dat[train,])
summary(svm.fit)
set.seed(100)
tune.out <- tune(svm, y ~., data=dat[train,],
kernel='radial',
ranges = list(cost=c(0.1,1,10,100,1000),
gamma=c(0.5, 1,2,3,4)))
summary(tune.out)
# show best model
tune.out$best.model
yhat <- predict(tune.out$best.model, dat[-train,])
confusionMatrix(yhat, dat[-train, 'y'])
install_load("ROCR")
# function to handle the different models
rocplot <- function(pred, truth, ...){
predob =  prediction(pred, truth)
perf = performance(predob, 'tpr', 'fpr')
plot(perf, ...)
}
svm.opt <- svm(y ~., data=dat[train,], kernel='radial',
gamma=2, cost=1, decision.values=T)
fitted <- attributes(predict(svm.opt, dat[train,], decision.values=T))$decision.values
rocplot(fitted, dat[train,'y'], main='Training Data')
attach(Auto)
attach(Auto); str(Auto)
head(Auto)
datatable(Auto)
datatable(Auto, filter = 'top',
options = list(pageLength = 5))
# Create a binary variable that takes on 1 for cars with gas mileage > median
Auto$y <- NA
Auto$y[Auto$mpg > median(Auto$mpg)] <- 1
Auto$y[Auto$mpg <= median(Auto$mpg)] <- 0
Auto$y <- as.factor(Auto$y)
length(Auto[is.na(Auto$y)]) # make sure there are no NA's
set.seed(123)
split <- createDataPartition(y=Auto$y, p=0.7, list=FALSE)
train <- Auto[split,]
test <- Auto[-split,]
# Remove mpg / name features
train <- train[-c(1,9)]
test <- test[-c(1,9)]
# 10 fold cross validation
ctr <- trainControl(method='repeatedcv',
number=10,
repeats=3)
# Recall as C increases, the margin tends to get wider
grid <- data.frame(C=seq(0.01,10,0.5))
svm.linear <- train(y ~., train,
method='svmLinear',
preProc=c('center','scale'),
trControl=ctr,
tuneGrid=grid)
svm.linear
svm.linear$bestTune
ggplot(svm.linear)
# Training error rate
confusionMatrix(predict(svm.linear, train), train$y)
# Testing error rate
yhat <- predict(svm.linear, test)
confusionMatrix(yhat, test$y)
set.seed(123)
# Try a polynomial function
svm.poly <- train(y ~., train,
method='svmPoly',
trControl=ctr,
tuneLength=4)
svm.poly
svm.poly$bestTune
plot(svm.poly)
# Training error rate
confusionMatrix(predict(svm.poly, train), train$y)
# Testing error rate
yhat <- predict(svm.poly, test)
confusionMatrix(yhat, test$y)
set.seed(123)
# Try a radial function
svm.radial <- train(y ~., train,
method='svmRadial',
trControl=ctr,
tuneLength=10)
svm.radial
svm.radial$bestTune
plot(svm.radial)
# Training error rate
confusionMatrix(predict(svm.radial, train), train$y)
# Testing error rate
yhat <- predict(svm.radial, test)
confusionMatrix(yhat, test$y)
data <- read.csv("https://raw.githubusercontent.com/sainsdataid/dataset/main/wine-quality-binary.csv")
str(data)
head(data)
# membuang kolom id
data$id <- NULL
# merubah kolom quality menjadi `factor`
data$quality <- as.factor(data$quality)
# melihat proporsi masing-masing kategori
quality <- data.frame(table(data$quality))
quality$prop <- round(quality$Freq/sum(quality$Freq), 3)
quality
# Membagi data Latih dan Data Uji
set.seed(123)  # Untuk menghasilkan nilai acak yang dapat direproduksi
# membagi data secara acak
# dengan menghasilkan indeks data latih dan data uji
# data latih = 75%, data uji = 25%
trainIndex <- createDataPartition(data$quality, p = 0.75, list = FALSE, times = 1)
# Buat data latih dan data uji berdasarkan indeks yang dihasilkan
data.train <- data[trainIndex, ]
data.test <- data[-trainIndex, ]
# melihat komposisi setiap kelas pada data train dan test
cbind("train" = table(data.train$quality), "test" = table(data.test$quality))
# Training SVM mod
svm.model <- svm(quality ~ ., data = data.train, kernel = "radial")
# Prediksi menggunakan data uji
predictions <- predict(svm.model , newdata = data.test)
# Hitung akurasi
accuracy <- mean(predictions == data.test$quality)
print(paste("Akurasi:", accuracy))
# untuk membuat confusion Matrix
confusionMatrix(data = predictions,
reference = data.test$quality)
svm.tuned <- tune(svm, quality ~ .,
data = data.train,
kernel = "radial",
ranges = list(cost = c( 0.1, 0.2, 0.5, 0.9, 1, 2, 10),
gamma=c(0.01,0.02, 0.05, 0.1)))
svm.tuned
(svm.best <- svm.tuned$best.model)
# Evaluasi model terbaik
best_predictions <- predict(svm.best, newdata = data.test)
# untuk membuat confusion Matrix
confusionMatrix(data = best_predictions,
reference = data.test$quality)
# Tuning hyperparameter (misalnya, mencari nilai C terbaik)
svm.tuned <- tune(svm, quality ~ ., data = data.train,
kernel = "polynomial",
cross = 3,
ranges = list(cost = c( 0.1, 0.2, 0.5, 0.9, 1, 2, 10),
gamma=c(0.01,0.02, 0.05, 0.1),
degree = c(1, 2, 3, 4))) # 1 sama dengan kernel linear
svm.tuned
(svm.best <- svm.tuned$best.model)
# Evaluasi model terbaik
best_predictions <- predict(svm.best, newdata = data.test)
# untuk membuat confusion Matrix
confusionMatrix(data = best_predictions,
reference = data.test$quality)
# Membaca data
data <- read.csv("https://raw.githubusercontent.com/sainsdataid/dataset/main/data_ipm_jawa_2018.csv")
# Kolom ID tidak diperlukan
data$id = NULL
# Melihat Struktur data
str(data)
# Training SVM model untuk regresi
svr.model <- svm(ipm_2018 ~ ., data = train_data, kernel = "radial", type = "eps-regression")
set.seed(123)
index <- createDataPartition(data$ipm_2018, p =  0.7, list = FALSE)
# Membagi dataset
train_data <- data[index, ]
test_data <- data[-index, ]
# Training SVM model untuk regresi
svr.model <- svm(ipm_2018 ~ ., data = train_data, kernel = "radial", type = "eps-regression")
# Prediksi menggunakan data uji
predictions <- predict(svr.model, newdata = test_data)
# Hitung Mean Squared Error (MSE)
rmse <- mean((predictions - test_data$ipm_2018)^2)^0.5
print(paste("Root Mean Squared Error (RMSE):", rmse))
# Tuning hyperparameter
svr.tuned <- tune(svm, ipm_2018 ~ .,
data = train_data,
kernel = "radial",
type = "eps-regression",
ranges = list(cost = c( 0.1, 0.2, 0.5, 0.9, 1, 2, 10),
gamma=c(0.01,0.02, 0.05, 0.1)))
# Model terbaik dari tuning
svr.best <- svr.tuned $best.model
# Evaluasi model terbaik
best_predictions <- predict(svr.best, newdata = test_data)
best_rmse <- mean((best_predictions- test_data$ipm_2018)^2)^0.5
print(paste("Mean Squared Error (MSE) Model Terbaik:", best_rmse))
install.packages('bookdown')

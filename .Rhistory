mode="max",
verbose=1,
)
# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
monitor="val_accuracy",
patience=30,
verbose=1,
)
# kompilasi model
model.3 %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = c("accuracy")
)
# melatih model
history <- model.3 %>% fit(
train_X, train_y,
shuffle = T,
epochs = 200,
batch_size = 64,
# validation_split = 0.2, # atau
validation_data = list(test_X, test_y),
callbacks=list(checkpoint, early_stopping),
)
summary(model.3)
summary(model.3)
# visualisasi history pelatihan
plot(history)
# evaluasi model
scores <- model.3 %>% evaluate(test_X, test_y)
print(scores)
# memuat model yang disimpan hasil checkpoint
last.model <- load_model_tf("model_check_x.keras")
pred.3 <- predict(last.model, test_X)
label_pred <- apply(pred.3, 1, which.max)
label_pred
# label_true <- apply(test_y, 1, which.max)
# label_true
confusionMatrix(as.factor(label_true), as.factor(label_pred))
data.biner <- data.frame(dt4)
str(data.biner)
to.binary <- function(level){
is.overweight <- (level %in% c("Overweight", "Obesity"))
as.integer(is.overweight)
}
# Mengubah 4 kelas menjadi 2 kelas (0: Tidak Overweight, 1: Overweight)
data.biner$NObeyesdad <- sapply(data.biner$NObeyesdad, to.binary)
str(data.biner)
# Membagi data menjadi data latih dan data uji
set.seed(111)
train.index <- createDataPartition(data.biner$NObeyesdad, p = 0.7, list = FALSE)
unique(dt4$NObeyesdad)
# Membagi data menjadi data latih dan data uji
set.seed(111)
train.index <- createDataPartition(data.biner$NObeyesdad, p = 0.7, list = FALSE)
unique(dt4$NObeyesdad)
View(dt4)
summary(data)
dt4 <- read.csv("https://raw.githubusercontent.com/Zen-Rofiqy/STA1382-TPM/main/Materi/Prak%2004/ObesityDataSet.csv", stringsAsFactors = TRUE)
datatable(dt4)
summary(data)
str(dt4)
summary(data)
summary(data)
summary(dt4)
colnames(dt4)
View(dt4)
as.data.frame(summary(dt4))
View(as.data.frame(summary(dt4)))
#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {
# convert arguments to vector
packages <- c(package1, ...)
# start loop to determine if each package is installed
for(package in packages){
# if package is installed locally, load
if(package %in% rownames(installed.packages()))
do.call('library', list(package))
# if package is not installed locally, download, then load
else {
install.packages(package)
do.call("library", list(package))
}
}
}
install_load("DT","dplyr","ggplot2","gridExtra","MASS","tree","rio","skimr")
theme1.1 <- list(
geom_hline(yintercept = 0, size = 1, colour="#333333"),
theme(axis.text.x = element_text(angle = 45, hjust = 1,
margin = margin(b = 10, t=-20)),
axis.text.y = element_text(vjust = 0.5, face = "bold",
margin = margin(l = 20, r = 0)),
plot.title = element_text(hjust = 0.5, face = "bold"),
text = element_text(size = 30),
plot.subtitle = element_text(hjust = 0.5),
panel.background = element_rect(fill = 'transparent'),
plot.background = element_rect(fill='transparent', color=NA),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()
)
)
skim(dt4)
skim(dt4)[1]
View(skim(dt4))
skim(dt4)
View(skim(dt4))
View(skim(dt4))
View(skim(dt4))
View(skim(dt4)[factor.top_counts])
View(skim(dt4)["factor.top_counts"])
View(skim(dt4)["factor.top_counts"][9])
View(skim(dt4)[9,"factor.top_counts"])
skim(dt4)
table(dt4$NObeyesdad)
library(tensorflow); library(keras); library(caret)
# One-Hot Encoder
for(i in 1:(dim(dt4)[2] - 1)){
if(is.factor(dt4[,i]) == TRUE){
dt4[,i] <- to_categorical(as.integer(dt4[,i]) - 1)
}
}
# Contoh Hasil One-Hot Encoder
head(dt4$MTRANS)
unique(dt4$NObeyesdad)
set.seed(123)
# Train-Testing Split
train.index <- createDataPartition(dt4$NObeyesdad, p = 0.8, list = F)
train <- dt4[train.index, ]
test <- dt4[-train.index, ]
# Feature Scaling (Min-Max)
preprocessParams <- preProcess(train[, -17], method = "range")
train_X <- as.matrix(predict(preprocessParams, train[, -17]))
test_X <- as.matrix(predict(preprocessParams, test[, -17]))
# Encoding Variabel Respons
train_y <- to_categorical(as.integer(train[, 17])-1)
test_y <- to_categorical(as.integer(test[, 17])-1)
table(dt4$NObeyesdad)
set.seed(123)
# Train-Testing Split
train.index <- createDataPartition(dt4$NObeyesdad, p = 0.8, list = F)
train <- dt4[train.index, ]
test <- dt4[-train.index, ]
# Feature Scaling (Min-Max)
preprocessParams <- preProcess(train[, -17], method = "range")
train_X <- as.matrix(predict(preprocessParams, train[, -17]))
test_X <- as.matrix(predict(preprocessParams, test[, -17]))
# Encoding Variabel Respons
train_y <- to_categorical(as.integer(train[, 17])-1)
test_y <- to_categorical(as.integer(test[, 17])-1)
data.biner <- data.frame(dt4)
str(data.biner)
to.binary <- function(level){
is.overweight <- (level %in% c("Overweight", "Obesity"))
as.integer(is.overweight)
}
# Mengubah 4 kelas menjadi 2 kelas (0: Tidak Overweight, 1: Overweight)
data.biner$NObeyesdad <- sapply(data.biner$NObeyesdad, to.binary)
str(data.biner)
table(data.biner$NObeyesdad)
data.biner <- data.frame(dt4)
str(data.biner)
table(data.biner$NObeyesdad)
to.binary <- function(level){
is.overweight <- (level %in% c("Overweight", "Obesity"))
as.integer(is.overweight)
}
# Mengubah 4 kelas menjadi 2 kelas (0: Tidak Overweight, 1: Overweight)
data.biner$NObeyesdad <- sapply(data.biner$NObeyesdad, to.binary)
str(data.biner)
table(data.biner$NObeyesdad)
# Membagi data menjadi data latih dan data uji
set.seed(111)
train.index <- createDataPartition(data.biner$NObeyesdad, p = 0.7, list = FALSE)
data.biner <- data.frame(dt4)
str(data.biner)
table(data.biner$NObeyesdad)
to.binary <- function(level){
is.overweight <- grepl("Overweight|Obesity", level)
as.integer(is.overweight)
}
# Mengubah 4 kelas menjadi 2 kelas (0: Tidak Overweight, 1: Overweight)
data.biner$NObeyesdad <- sapply(data.biner$NObeyesdad, to.binary)
str(data.biner)
table(data.biner$NObeyesdad)
# Membagi data menjadi data latih dan data uji
set.seed(111)
train.index <- createDataPartition(data.biner$NObeyesdad, p = 0.7, list = FALSE)
train <- data.biner[train.index, ]
test <- data.biner[-train.index, ]
# Features Scaling MinMax (0, 1)
preprocessParams <- preProcess(train[, -17], method=c("range"))
train_X <- as.matrix(predict(preprocessParams, train[, -17]))
test_X <- as.matrix(predict(preprocessParams, test[, -17]))
# Peubah target sudah dalam bentuk 0 dan 1
# (tidak perlu one-hot encoding)
train_y = train[, 17]
test_y = test[, 17]
head(train_y)
# Binary Model dengan 2 hidden layers + dropout + checkpoint + earlyStopping
model.bin <- keras_model_sequential() %>%
layer_dense(units = 120, activation = "relu", input_shape = ncol(train_X)) %>%
layer_dropout(0.2) %>%
layer_dense(units = 120, activation = "relu") %>%
layer_dropout(0.1) %>%
layer_dense(units = 1, activation = "sigmoid")   # units = 1 dan fungsi aktivasi = sigmoid
# menentukan nama dan path file untuk penyimpanan model
filepath <- "model_bin_check.keras"
# mengatur kriteria checkpoint
# simpan model jika memperoleh skor terbaik
checkpoint <- callback_model_checkpoint(
filepath=filepath,
monitor="val_accuracy",
save_best_only=TRUE,
mode="max",
verbose=1,
)
# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
monitor="val_accuracy",
patience=30,
verbose=1,
)
# kompilasi model
model.bin %>% compile(
loss = "binary_crossentropy",    # loss : binary_cross_entropy
optimizer = "adam",
metrics = c("accuracy")
)
# melatih model
history <- model.bin %>% fit(
train_X, train_y,
shuffle = T,
epochs = 200,
batch_size = 64,
# validation_split = 0.2, # atau
validation_data = list(test_X, test_y),
callbacks=list(checkpoint, early_stopping),
)
# melatih model
history <- model.bin %>% fit(
train_X, train_y,
shuffle = T,
epochs = 200,
batch_size = 64,
verbose=0, # Hide hasil
# validation_split = 0.2, # atau
validation_data = list(test_X, test_y),
callbacks=list(checkpoint, early_stopping),
)
# Binary Model dengan 2 hidden layers + dropout + checkpoint + earlyStopping
model.bin <- keras_model_sequential() %>%
layer_dense(units = 120, activation = "relu", input_shape = ncol(train_X)) %>%
layer_dropout(0.2) %>%
layer_dense(units = 120, activation = "relu") %>%
layer_dropout(0.1) %>%
layer_dense(units = 1, activation = "sigmoid")   # units = 1 dan fungsi aktivasi = sigmoid
# menentukan nama dan path file untuk penyimpanan model
filepath <- "model_bin_check.keras"
# mengatur kriteria checkpoint
# simpan model jika memperoleh skor terbaik
checkpoint <- callback_model_checkpoint(
filepath=filepath,
monitor="val_accuracy",
save_best_only=TRUE,
mode="max",
verbose=1,
)
# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
monitor="val_accuracy",
patience=30,
verbose=1,
)
# kompilasi model
model.bin %>% compile(
loss = "binary_crossentropy",    # loss : binary_cross_entropy
optimizer = "adam",
metrics = c("accuracy")
)
# melatih model
history <- model.bin %>% fit(
train_X, train_y,
shuffle = T,
epochs = 200,
batch_size = 64,
verbose=0, # Hide hasil
# validation_split = 0.2, # atau
validation_data = list(test_X, test_y),
callbacks=list(checkpoint, early_stopping),
)
summary(model.bin)
# evaluasi model
scores <- model.bin %>% evaluate(test_X, test_y)
print(scores)
plot(history)
# memuat model yang disimpan hasil checkpoint
model.bin.check <- load_model_tf("model_bin_check.keras")
# prediksi
pred.bin <- predict(model.bin.check, test_X)
# karena hasilnya berupa nilai 0-1, dengan cut-off 0.5 kita bisa menggunakan fungsi round saja
label_pred <- round(pred.bin)
confusionMatrix(as.factor(test_y), as.factor(label_pred))
# Menambahkan Kolom 'Age'
data.ab$Age <- data.ab$Rings + 1.5
data.ab <-  read.csv("https://raw.githubusercontent.com/Zen-Rofiqy/STA1382-TPM/main/Materi/Prak%2004/abalone.csv", stringsAsFactors = TRUE)
datatable(data.ab)
str(data.ab)
# evaluasi model
scores <- model.bin %>% evaluate(test_X, test_y)
skim(data.ab)
# Menambahkan Kolom 'Age'
data.ab$Age <- data.ab$Rings + 1.5
# Menghapus Kolom 'Rings'
data.ab$Rings <- NULL
# One-Hot Encoding Kolom 'Sex'
data.ab$Sex <- to_categorical(as.integer(data.ab$Sex) - 1)
set.seed(123)
train.index <- createDataPartition(data.ab$Age, p = 0.8, list = FALSE)
train <- data.ab[train.index, ]
test <- data.ab[-train.index, ]
# Melakukan Feature Scaling min max (0, 1)
preprocessParams <- preProcess(train[, -9], method=c("range"))
train_X <- as.matrix(predict(preprocessParams, train[, -9]))
test_X <- as.matrix(predict(preprocessParams, test[, -9]))
train_y <- train[, 9]
test_y <- test[, 9]
# Membuat model neural network dengan 2 hidden layer
model.reg <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(train_X)) %>%
layer_dropout(0.2) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(units = 1, activation = "linear")      # units = 1 dan activation = "linear"
# menentukan nama dan path file untuk penyimpanan model
filepath <- "model_reg_check.keras"
# mengatur kriteria checkpoint
# simpan model jika memperoleh skor terbaik
checkpoint <- callback_model_checkpoint(
filepath=filepath,
monitor="val_loss",
save_best_only=TRUE,
mode="min",           # minimum loss
verbose=1,
)
# mengatur kondisi untuk early stopping
early_stopping <- callback_early_stopping(
monitor="val_loss",
patience=15,
verbose=1,
)
# Kompilasi model
model.reg %>% compile(
loss = "mean_squared_error",
optimizer = "adam",
metrics = list("mean_squared_error", "mean_absolute_error")
)
# Melakukan tahapan pelatihan model
history <- model.reg %>% fit(
train_X, train_y,
shuffle = T,
epochs = 100,
batch_size = 32,
validation_split = 0.2,
callbacks=list(checkpoint, early_stopping),
verbose = F
)
# Mengevaluasi model menggunakan data uji
scores <- model.reg %>% evaluate(test_X, test_y)
print(scores)
summary(model.reg)
plot(history)
plot(history)
cat("RMSE:", scores[2]^0.5, "\n")
cat("MAE:", scores[3], "\n")
# Melakukan prediksi
prediksi <- predict(model.reg, test_X)
# menampilkan 10 hasil prediksi pertama
# disandingkan dengan nilai aslinya
head(cbind("True" = test_y, "Pred"= prediksi), 20)
# Mengevaluasi model menggunakan data uji
scores <- model.reg %>% evaluate(test_X, test_y)
print(scores)
keras_train <- model.reg %>% predict(train_X)
keras_test <- model.reg %>% predict(test_X)
# Training Evaluation
postResample(keras_train[,1], train$Age)
# Testing Evaluation
postResample(keras_test[,1], test$Age)
#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {
# convert arguments to vector
packages <- c(package1, ...)
# start loop to determine if each package is installed
for(package in packages){
# if package is installed locally, load
if(package %in% rownames(installed.packages()))
do.call('library', list(package))
# if package is not installed locally, download, then load
else {
install.packages(package)
do.call("library", list(package))
}
}
}
install_load("DT","dplyr","ggplot2","gridExtra","MASS","tree","rio","skimr")
theme1.1 <- list(
geom_hline(yintercept = 0, size = 1, colour="#333333"),
theme(axis.text.x = element_text(angle = 45, hjust = 1,
margin = margin(b = 10, t=-20)),
axis.text.y = element_text(vjust = 0.5, face = "bold",
margin = margin(l = 20, r = 0)),
plot.title = element_text(hjust = 0.5, face = "bold"),
text = element_text(size = 30),
plot.subtitle = element_text(hjust = 0.5),
panel.background = element_rect(fill = 'transparent'),
plot.background = element_rect(fill='transparent', color=NA),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()
)
)
#install.packages("tensorflow")
#install.packages("keras")
#library("reticulate")
#virtualenv_create("r-reticulate2", python = install_python())
#tensorflow::install_tensorflow()
#keras::install_keras()
# Delete Installation
#wunlink("~/.virtualenvs/r-tensorflow", recursive = TRUE)
library(tensorflow); library(keras); library(caret)
suppressMessages({
library(e1071)
library(caret)
})
#                      -=( Install & Load Package Function )=-
install_load <- function (package1, ...)  {
# convert arguments to vector
packages <- c(package1, ...)
# start loop to determine if each package is installed
for(package in packages){
# if package is installed locally, load
if(package %in% rownames(installed.packages()))
do.call('library', list(package))
# if package is not installed locally, download, then load
else {
install.packages(package)
do.call("library", list(package))
}
}
}
install_load("DT","dplyr","ggplot2","gridExtra","MASS","tree","rio","skimr",
"tidyverse","kernlab","e1071","ISLR","RColorBrewer")
theme1.1 <- list(
geom_hline(yintercept = 0, size = 1, colour="#333333"),
theme(axis.text.x = element_text(angle = 45, hjust = 1,
margin = margin(b = 10, t=-20)),
axis.text.y = element_text(vjust = 0.5, face = "bold",
margin = margin(l = 20, r = 0)),
plot.title = element_text(hjust = 0.5, face = "bold"),
text = element_text(size = 30),
plot.subtitle = element_text(hjust = 0.5),
panel.background = element_rect(fill = 'transparent'),
plot.background = element_rect(fill='transparent', color=NA),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank()
)
)
#install.packages("tensorflow")
#install.packages("keras")
#library("reticulate")
#virtualenv_create("r-reticulate2", python = install_python())
#tensorflow::install_tensorflow()
#keras::install_keras()
# Delete Installation
#wunlink("~/.virtualenvs/r-tensorflow", recursive = TRUE)
library(tensorflow); library(keras); library(caret)
suppressMessages({
library(e1071)
library(caret)
})
set.seed(10)
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))
# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) +
geom_point(size = 2) +
scale_color_manual(values=c("#000000", "#FF0000")) +
theme(legend.position = "none")
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit, dat)
# Construct sample data set - not completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))
# Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) +
geom_point(size = 2) +
scale_color_manual(values=c("#000000", "#FF0000")) +
theme(legend.position = "none")
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", cost = 10)
# Plot Results
plot(svmfit, dat)
svmfit2 <- svm(y~., data = dat, kernel = "polynomial", gamma = 2, cost = 10)
# Plot Results
plot(svmfit2, dat)
